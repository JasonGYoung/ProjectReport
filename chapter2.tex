\chapter{Background and Related Works}
The purpose of code generation tools is to help developers improve their productivity, ensure correctness of code syntax, and lessen the number of errors in software \cite{groher_01}.  The input specifications of these tools introduce additional levels of indirection to solve these problems \cite{spinellis_01}.  A range of tools have been created to generate code that can be complex and tedious to write by hand:
\begin{itemize}
  \item Scanners
  \item Parsers
  \item Interpreters
  \item Compilers
  \item Graphical user interfaces
\end{itemize}

\indent
The TEBNF input specification shares some similarities to those accepted by other tools.  In order to understand how TEBNF and the TEBNF code generation tool improves upon these these approaches, a review of pertinent code generation tools and their input specifications is helpful.

\section{Traditional Scanner and Parser Code Generation}
Yacc (“Yet Another Compiler Compiler”) \cite{johnson_01} is a popular tool first introduced in 1975 that generates LALR parsers given (1) a context-free grammar to describe input, (2) an action (code snippets) to run for each token grouping that matches a grammar rule, and (3) code to provide input tokens to the parser.  This input specification is a hybrid between a declarative domain-specific language (i.e. a grammar) and an imperative programming language like C \cite{demetrescu_01,lloyd_01,gifford_01}.  Nevertheless, Yacc lacks the ability to read an input stream and convert it into tokens for parsing.

\indent
Lexical analyzer generators such as Lex and Flex must be used in conjunction with Yacc to accomplish this separate task of lexical analyzer generation \cite{johnson_01,lesk_01}.  The practice of using separate tools to generate a lexical analyzer and parser means that input formats for each tool must be learned and used correctly to achieve the desired result.  A benefit of the TEBNF input specification  is that it describes how data is received and parsed (appendix~\ref{appendix:TEBNF}), allowing the TEBNF code generation tool to generate both lexical analysis and parsing code.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/YaccGrammarRule.png}
\caption{Example Yacc grammar rule matched to input.}
\label{fig:YaccGrammarRule}
\end{figure}

\indent
Figure~\ref{fig:YaccGrammarRule} shows an example Yacc grammar rule matched to input formatted as military time.  In this example, military\_hour, minute, and second (all defined somewhere else in the grammar) describe specific parts of input data as military\_time.  When tokens match this rule, a code action is executed \cite{johnson_01,niemann_01}.

\indent
Similar to Yacc is Bison \cite{aycock_01,demaille_01}, a Yacc-compatible parser generator that accepts any properly written Yacc grammar.  Like Yacc, Bison-generated parsers read a sequence of tokens from a scanner generated by a lexical analyzer generator like Lex or flex.

\indent
To illustrate the steps of traditional parser generation using Lex and Yacc (figure ~\ref{fig:LexAndYaccCompileSequence}) \cite{johnson_01,lesk_01,niemann_01}, a file is provided by the developer containing a set of patterns that define how to separate strings found in source data.  This file is read by Lex, which uses these patterns to generate the C source code of a lexical analyzer.  This newly generated lexical analyzer uses the patterns to identify specific strings in the input and split them into tokens to simplify processing.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/LexAndYaccCompileSequence.png}
\caption[Lex and Yacc Compilation Sequence]{Lex and Yacc Compilation Sequence \cite{niemann_01}.}
\label{fig:LexAndYaccCompileSequence}
\end{figure}

\indent
A file containing grammar rules is provided by the developer to Yacc, which uses those rules to generate C source code for a syntax analyzer (i.e. parser).  The syntax analyzer uses this grammar to transform the tokens output by the lexical analyzer into a syntax tree.  The structure of this syntax tree implies the precedence and associativity of operators found within the tokens.  The syntax tree is then traversed in depth-first order to generate the desired source code (figure ~\ref{fig:LexAndYaccCompileSequence}) \cite{niemann_01}.

\indent
A predicated-LL(k) parser called ANTLR \cite{parr_01} was introduced by Parr and Quong in 1995.  The ANTLR parser generator was advertised to be easier to use than generators like Yacc or Bison.  An LL(k) parser is a top-down parser that parses from left to right, utilizing a look-ahead of k tokens.  All parsing decisions are made solely from the next k tokens, which means that it does no backtracking.

\indent
The Hyacc (Hawaii Yacc) parser generator first released in 2008 supports complete LR(0), LALR(1), LR(1), and partial LR(k) \cite{chen_01,chen_02}.  Hyacc is compatible with Yacc and Bison input grammars and works with Lex.  The Hyacc parser generator is notable because it can resolve reduce/reduce conflicts through its implementation of the LR(1) parser generation algorithm \cite{chen_01}.  Reduce/reduce conflicts occur when two or more rules in an input grammar apply to the same input sequence \cite{free_01}.  These conflicts are typically the result of a serious problem with an input grammar \cite{free_01}.

\indent
The traditional code generation process using a lexical analyzer generator in conjunction with a parser generator and actions code is shown in figure~\ref{fig:TraditionalCodeGenProcess}.  This process illustrates the pattern used by many parser generation tools.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/TraditionalCodeGenProcess.png}
\caption{Traditional code generation process.}
\label{fig:TraditionalCodeGenProcess}
\end{figure}

\indent
All of these parser generators provide an effective means to reduce human interaction with code.  They have the added benefit of generating logical and syntactically correct code as long as the grammar is correct.  On the other hand, the input grammars used by these parser generators cannot be used for lexical analysis of the parser input.  Code actions that generate output must be manually written and inserted into the generated code.  The inconvenience of editing generated code is avoidable in TEBNF because actions are defined directly within TEBNF grammars (appendix~\ref{appendix:TEBNF}).

\section{Model-Based Parser Code Generation}

\indent
Model-based parser generators provide an alternative to traditional parser generators using a model-based language specification.  This kind of specification is explained by \cite{quesada_01} and \cite{quesada_02} as starting with an abstract syntax model (ASM) embodying the main concepts of a given language.  One or more concrete syntax models (CSMs) are created from this ASM.  Each CSM defines specific details about the language being modelled.  Elements within the ASM can then be converted into their concrete representation using a mapping of the ASM to its CSM(s).  This mapping is created by annotating the ASM with pertinent constraint metadata.  With this mapping in place, any changes to the ASM by the user will cause the language processor to automatically update to reflect those changes.  Figure~\ref{fig:ModelBasedCodeGenProcess} illustrates the model-based parser code generation process.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/ModelBasedCodeGenProcess.png}
\caption[Model-based code generation process.]{Model-based code generation process, based on a figure from \cite{quesada_02}.}
\label{fig:ModelBasedCodeGenProcess}
\end{figure}

\indent
Since grammar specifications are not needed by model-based parser generators, they can offer several advantages over traditional parser generators \cite{quesada_02}:
\begin{itemize}
  \item An easier language design process.  Language design is decoupled from language processing because the language grammar is automatically generated.
  \item Non-tree structures can be modeled.  This is different from traditional parser generators that force users to model a tree structure.
  \item Some semantic checks like reference resolution can be automated.
  \item Handles references between language elements, as opposed to the traditional way of resolving references manually using a symbol table.
\end{itemize}

\indent
Model-based parser generators can behave in one of two ways based on the ASM used \cite{quesada_02}: (1) as a traditional parser generator when an ASM representing a tree structure is used, or (2) as a model-based parser generator when an ASM representing a non-tree structure is used.   The second possibility causes a model-based parser generator to begin the reference resolution process, resulting in an abstract syntax graph.

\indent
ModelCC is a model-based parser generator that accepts an annotated conceptual model as its input rather than a context-free grammar \cite{quesada_01}.  ModelCC uses this annotated model to generate a parser written in Java that automatically instantiates the language conceptual model \cite{quesada_01, modelcc_01}.

\indent
The process of generating a parser using ModelCC as explained by \cite{quesada_02} begins with an ASM that is defined by classes representing language elements and the relationships between them.  Metadata annotations are added to these language elements and their corresponding relationships to produce an ASM to CSM mapping.  Reference resolution is performed to match referenced objects to their equivalent object instantiations.  A parser is then generated that automatically instantiates the conceptual model.

\indent
ModelCC decreases human interaction with code by generating grammar code and lexical analysis code from an input model specification.  Yet it is similar to other parser generators because it does not implicitly support specific input and output methods such as UDP/IP, etc.  The TEBNF code generation tool differs from ModelCC (and the other tools reviewed in this chapter) because it is able to specify several input and output methods (appendix~\ref{appendix:TEBNF}).

